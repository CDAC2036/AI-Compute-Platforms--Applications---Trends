{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16c82ca3",
   "metadata": {},
   "source": [
    "# K-means training\n",
    "\n",
    "- Now that the RDD is ready for training, in the second part of the exercise, you'll train the RDD with PySpark's MLlib's KMeans algorithm. The algorithm is somewhat naive--it clusters the data into k clusters, even if k is not the right number of clusters to use. Therefore, when using k-means clustering, the most important parameter is a target number of clusters to generate, k. In practice, you rarely know the “true” number of clusters in advance, so the best practice is to try several values of k until the average intracluster distance stops decreasing dramatically\n",
    "\n",
    "- In this 2nd part, you'll test with k's ranging from 13 to 16 and use the [elbow](https://bl.ocks.org/rpgove/0060ff3b656618e9136b) method to chose the correct k. The idea of the `elbow` method is to run k-means clustering on the dataset for a range of values of k, calculate Within Set Sum of Squared Error (WSSSE, this function is already provided to you) and select the best k based on the sudden drop in WSSSE. Finally, you'll retrain the model with the best k (15 in this case) and print the centroids (cluster centers).\n",
    "\n",
    "- Remember, you already have a SparkContext `sc` and `rdd_split_int` RDD available in your workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a17f265",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "- Train the KMeans model with clusters from 13 to 16 and print the WSSSE for each cluster.\n",
    "- Train the KMeans model again with the best cluster (k=15).\n",
    "- Get the Cluster Centers (centroids) of KMeans model trained with k=15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de212f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc0b6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef0ec361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5000 rows in the rdd_split_int dataset\n"
     ]
    }
   ],
   "source": [
    "file_path = 'file:///home/talentum/test-jupyter/P2/M4/SM4/4_Clustering/Dataset/5000_points.txt'\n",
    "\n",
    "# Load the dataset into a RDD\n",
    "clusterRDD = sc.textFile(file_path)\n",
    "\n",
    "# Split the RDD based on tab\n",
    "rdd_split = clusterRDD.map(lambda x: x.split('\\t'))\n",
    "\n",
    "# Transform the split RDD by creating a list of integers\n",
    "rdd_split_int = rdd_split.map(lambda x: [int(x[0]), int(x[1])])\n",
    "\n",
    "# Count the number of rows in RDD \n",
    "print(\"There are {} rows in the rdd_split_int dataset\".format(rdd_split_int.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72d56974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cluster 13 has Within Set Sum of Squared Error 249164132.49410182\n",
      "The cluster 14 has Within Set Sum of Squared Error 209371154.24941802\n",
      "The cluster 15 has Within Set Sum of Squared Error 169394691.52639425\n",
      "The cluster 16 has Within Set Sum of Squared Error 202384225.6640126\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.clustering import KMeans\n",
    "import math\n",
    "\n",
    "def error(point):\n",
    "    center = model.centers[model.predict(point)]\n",
    "    return math.sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "# Train the model with clusters from 13 to 16 and compute WSSSE \n",
    "for clst in range(13, 17):\n",
    "    model = KMeans.train(rdd_split_int, clst, seed=1)\n",
    "    WSSSE = rdd_split_int.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "    print(\"The cluster {} has Within Set Sum of Squared Error {}\".format(clst, WSSSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03bc030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model again with the best k \n",
    "model = KMeans.train(rdd_split_int, k=15, seed=1)\n",
    "\n",
    "# Get cluster centers\n",
    "cluster_centers = model.clusterCenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a938867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([670929.06818182, 862765.73295455]),\n",
       " array([852058.45259939, 157685.52293578]),\n",
       " array([858947.9713467 , 546259.65902579]),\n",
       " array([398555.94857143, 404855.06857143]),\n",
       " array([507818.31339031, 175610.41595442]),\n",
       " array([801616.78164557, 321123.34177215]),\n",
       " array([606574.95622896, 574455.16835017]),\n",
       " array([320602.55, 161521.85]),\n",
       " array([337565.11890244, 562157.17682927]),\n",
       " array([167856.14071856, 347812.71556886]),\n",
       " array([244654.8856305 , 847642.04105572]),\n",
       " array([617601.91071429, 399504.21428571]),\n",
       " array([139682.37572254, 558123.40462428]),\n",
       " array([823421.2507837 , 731145.27272727]),\n",
       " array([417799.69426752, 787001.99363057])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d617857",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
