{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c67d0ede",
   "metadata": {},
   "source": [
    "# Print word frequencies\n",
    "\n",
    "# Print word frequencies\n",
    "\n",
    "- After combining the values (counts) with the same key (word), you'll print the word frequencies using the `take(N)` action. You could have used the `collect()` action but as a best practice, it is not recommended as `collect()` returns all the elements from your RDD. You'll use `take(N)` instead, to return N elements from your RDD.\n",
    "\n",
    "- What if we want to return the top 10 words? For this first, you'll need to swap the key (word) and values (counts) so that keys is count and value is the word. After you swap the key and value in the tuple, you'll sort the pair RDD based on the key (count) and print the top 10 words in descending order.\n",
    "\n",
    "- You already have a `SparkContext` `sc` and `resultRDD` available in your workspace.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3344e9ec",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "- Print the first 10 words and their frequencies from the `resultRDD`.\n",
    "- Swap the keys and values in the `resultRDD`.\n",
    "- Sort the keys according to descending order.\n",
    "- Print the top 10 most frequent words and their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de212f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc0b6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07f02448",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['i',\n",
    " 'me',\n",
    " 'my',\n",
    " 'myself',\n",
    " 'we',\n",
    " 'our',\n",
    " 'ours',\n",
    " 'ourselves',\n",
    " 'you',\n",
    " 'your',\n",
    " 'yours',\n",
    " 'yourself',\n",
    " 'yourselves',\n",
    " 'he',\n",
    " 'him',\n",
    " 'his',\n",
    " 'himself',\n",
    " 'she',\n",
    " 'her',\n",
    " 'hers',\n",
    " 'herself',\n",
    " 'it',\n",
    " 'its',\n",
    " 'itself',\n",
    " 'they',\n",
    " 'them',\n",
    " 'their',\n",
    " 'theirs',\n",
    " 'themselves',\n",
    " 'what',\n",
    " 'which',\n",
    " 'who',\n",
    " 'whom',\n",
    " 'this',\n",
    " 'that',\n",
    " 'these',\n",
    " 'those',\n",
    " 'am',\n",
    " 'is',\n",
    " 'are',\n",
    " 'was',\n",
    " 'were',\n",
    " 'be',\n",
    " 'been',\n",
    " 'being',\n",
    " 'have',\n",
    " 'has',\n",
    " 'had',\n",
    " 'having',\n",
    " 'do',\n",
    " 'does',\n",
    " 'did',\n",
    " 'doing',\n",
    " 'a',\n",
    " 'an',\n",
    " 'the',\n",
    " 'and',\n",
    " 'but',\n",
    " 'if',\n",
    " 'or',\n",
    " 'because',\n",
    " 'as',\n",
    " 'until',\n",
    " 'while',\n",
    " 'of',\n",
    " 'at',\n",
    " 'by',\n",
    " 'for',\n",
    " 'with',\n",
    " 'about',\n",
    " 'against',\n",
    " 'between',\n",
    " 'into',\n",
    " 'through',\n",
    " 'during',\n",
    " 'before',\n",
    " 'after',\n",
    " 'above',\n",
    " 'below',\n",
    " 'to',\n",
    " 'from',\n",
    " 'up',\n",
    " 'down',\n",
    " 'in',\n",
    " 'out',\n",
    " 'on',\n",
    " 'off',\n",
    " 'over',\n",
    " 'under',\n",
    " 'again',\n",
    " 'further',\n",
    " 'then',\n",
    " 'once',\n",
    " 'here',\n",
    " 'there',\n",
    " 'when',\n",
    " 'where',\n",
    " 'why',\n",
    " 'how',\n",
    " 'all',\n",
    " 'any',\n",
    " 'both',\n",
    " 'each',\n",
    " 'few',\n",
    " 'more',\n",
    " 'most',\n",
    " 'other',\n",
    " 'some',\n",
    " 'such',\n",
    " 'no',\n",
    " 'nor',\n",
    " 'not',\n",
    " 'only',\n",
    " 'own',\n",
    " 'same',\n",
    " 'so',\n",
    " 'than',\n",
    " 'too',\n",
    " 'very',\n",
    " 'can',\n",
    " 'will',\n",
    " 'just',\n",
    " 'don',\n",
    " 'should',\n",
    " 'now']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87d70cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Project', 9)\n",
      "('EBook', 1)\n",
      "('Shakespeare', 12)\n",
      "('', 65498)\n",
      "('use', 38)\n",
      "('anyone', 1)\n",
      "('anywhere', 1)\n",
      "('restrictions', 1)\n",
      "('whatsoever.', 1)\n",
      "('may', 162)\n",
      " has 65498 counts\n",
      "thou has 650 counts\n",
      "thy has 574 counts\n",
      "shall has 393 counts\n",
      "would has 311 counts\n",
      "good has 295 counts\n",
      "thee has 286 counts\n",
      "love has 273 counts\n",
      "Enter has 269 counts\n",
      "th' has 254 counts\n"
     ]
    }
   ],
   "source": [
    "file_path = \"file:///home/talentum/test-jupyter/P2/M2/SM4/Dataset/Complete_Shakespeare.txt\"\n",
    "\n",
    "# Create a baseRDD from the file path\n",
    "baseRDD = sc.textFile(file_path)\n",
    "\n",
    "# Split the lines of baseRDD into words\n",
    "splitRDD = baseRDD.flatMap(lambda x: x.split(' '))\n",
    "\n",
    "# Convert the words in lower case and remove stop words from stop_words\n",
    "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\n",
    "\n",
    "# Create a tuple of the word and 1 \n",
    "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n",
    "\n",
    "# Count of the number of occurences of each word\n",
    "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Display the first 10 words and their frequencies\n",
    "for word in resultRDD.take(10):\n",
    "    print(word)\n",
    "\n",
    "# Swap the keys and values \n",
    "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "# Sort the keys in descending order\n",
    "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n",
    "\n",
    "# Show the top 10 most frequent words and their frequencies\n",
    "for word in resultRDD_swap_sort.take(10):\n",
    "    print(\"{} has {} counts\". format(word[1], word[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c0513f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('of', 493)\n",
      "('United', 85)\n",
      "('in', 137)\n",
      "('Order', 1)\n",
      "('form', 1)\n",
      "('more', 10)\n",
      "('perfect', 1)\n",
      "('Union,', 4)\n",
      "('establish', 4)\n",
      "('Tranquility,', 1)\n",
      "the has 662 counts\n",
      "of has 493 counts\n",
      "shall has 293 counts\n",
      "and has 256 counts\n",
      "to has 183 counts\n",
      "be has 178 counts\n",
      "or has 157 counts\n",
      "in has 137 counts\n",
      "by has 100 counts\n",
      "a has 94 counts\n"
     ]
    }
   ],
   "source": [
    "file_path = \"file:///home/talentum/test-jupyter/P2/M2/SM4/constitution.txt\"\n",
    "\n",
    "# Create a baseRDD from the file path\n",
    "baseRDD = sc.textFile(file_path)\n",
    "\n",
    "# Split the lines of baseRDD into words\n",
    "splitRDD = baseRDD.flatMap(lambda x: x.split(' '))\n",
    "\n",
    "# Convert the words in lower case\n",
    "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower())\n",
    "\n",
    "# Create a tuple of the word and 1 \n",
    "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n",
    "\n",
    "# Count of the number of occurences of each word\n",
    "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Display the first 10 words and their frequencies\n",
    "for word in resultRDD.take(10):\n",
    "    print(word)\n",
    "\n",
    "# Swap the keys and values \n",
    "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "# Sort the keys in descending order\n",
    "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n",
    "\n",
    "# Show the top 10 most frequent words and their frequencies\n",
    "for word in resultRDD_swap_sort.take(10):\n",
    "    print(\"{} has {} counts\". format(word[1], word[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af92fe80",
   "metadata": {},
   "source": [
    "**Step - 1:** Create a text file using ```nano shakespeare.py```, also watch the location. Always create shebang statement at the beginning of the file by using (```#!```) shebang, else it will return ASCII text when run command ```file <filename>``` and use ```which python``` to get path from where shell is running python.\n",
    "\n",
    "**Step - 2:** Unset the jupyter file either you can use $ and do it or run ```source unset_jupyter.sh``` by being in the location where ```unset_jupyter.sh``` is saved.\n",
    "\n",
    "**Step - 3:** to see what is in ```unset_jupyter.sh``` use ```cat unset_jupyter.sh```.\n",
    "\n",
    "**Step - 4:** run the file using below command and watch out for location\n",
    "```spark-submit [options] <app jar | python file | R file> [app arguments]```\n",
    "\n",
    "Note: use ```echo $?``` to know exit status of the previous command. If it is 1 then error is occured, elsewise command executed successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa1b1b7",
   "metadata": {},
   "source": [
    "The find command lets you efficiently search for files, folders, and character and block devices.\n",
    "\n",
    "Syntax:\n",
    "\n",
    "```find /path/ -type f -name file-to-search```\n",
    "\n",
    "Where,\n",
    "\n",
    "/path is the path where file is expected to be found. This is the starting point to search files. The path can also be/or . which represent root and current directory, respectively.\n",
    "-type represents the file descriptors. They can be any of the below:\n",
    "f – Regular file such as text files, images and hidden files.\n",
    "\n",
    "d – Directory. These are the folders under consideration.\n",
    "\n",
    "l – Symbolic link. Symbolic links point to files and are similar to shortcuts.\n",
    "\n",
    "c – Character devices. Files that are used to access character devices are called character device files. Drivers communicate with character devices by sending and receiving single characters (bytes, octets).  Examples include     keyboards, sound cards and mouse.\n",
    "\n",
    "b – Block devices. Files that are used to access block devices are called block device files. Drivers communicate with block devices by sending and receiving entire blocks of data. Examples include USB, CD-ROM\n",
    "\n",
    "-name is the name of the file type that you want to search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
